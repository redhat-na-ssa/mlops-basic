{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f32de8-dcc0-44ed-bae3-5264a2bba0dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Project Goal\n",
    "    \n",
    "Build a basic Tensorflow Pipeline that automatically executes tasks from ingestion to serving. Scenario, uses sythentically generated patient data (Heart Rate, Temperature, Respiratory Rate, White Blood Cell Count) that has been labelled with a 1 (Septic) or 0 (Not-Septic). reate and run a TFX pipeline to train a model to predict septic patients based on biological markers. The pipeline will consist of three essential TFX components: ExampleGen, Trainer and Pusher. The pipeline includes the most minimal ML workflow like importing data, training a model and exporting the trained TFRS ranking model.\n",
    "\n",
    "## Overview of steps\n",
    "1. Install required software\n",
    "1. Configure pipeline variables\n",
    "1. Prepare the raw data \n",
    "1. Write the training pipeline (data ingestion, model training, model pushing)\n",
    "1. Run the training pipeline\n",
    "1. Push the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a899d05-a596-4a4a-989a-14014f9f7f28",
   "metadata": {},
   "source": [
    "# Import required software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8848b2e-b6da-4344-9e7b-1bd47ba432e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out due to ODH notebook image error on direct pip commands. Use %horus error.\n",
    "# run below commands in terminal\n",
    "#!pip install --upgrade pip\n",
    "#!pip install tensorflow tfx --progress-bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388e033a-334a-44e2-af85-da7c48ada7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "TFX version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfef015-2b7f-435d-8480-78c74568de94",
   "metadata": {},
   "source": [
    "# Configure Pipeline Variables\n",
    "\n",
    "There are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory. Instead of using the SchemaGen component to generate a schema, for this tutorial we will create a hardcoded schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e3dea5-08b4-48be-9428-59699ff9be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"basic_pipeline\"\n",
    "MODEL_NAME = \"sepsis\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('../pipeline', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('../pipeline', 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('../models', MODEL_NAME)\n",
    "\n",
    "# Path to the training data\n",
    "DATA_TRAIN = '../data/training_data'\n",
    "# File name\n",
    "TRAIN='septic_data_labelled.csv'\n",
    "RAW_DATA = os.path.join(DATA_TRAIN,TRAIN)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a585f2-451a-4a03-9bf1-24745b5042cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pipeline/basic_pipeline\n",
      "../pipeline/metadata.db\n",
      "../models/sepsis\n",
      "../data/training_data\n",
      "../data/training_data/septic_data_labelled.csv\n"
     ]
    }
   ],
   "source": [
    "print(PIPELINE_ROOT)\n",
    "print(METADATA_PATH)\n",
    "print(SERVING_MODEL_DIR)\n",
    "print(DATA_TRAIN)\n",
    "print(RAW_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364db1b-4c3b-4085-9b1d-ae8ffd0e04c5",
   "metadata": {},
   "source": [
    "# Prepare and examine the training data\n",
    "\n",
    "There are four numeric features in this dataset:\n",
    "\n",
    "Heart Rate\n",
    "Temperature\n",
    "Respiratory Rate\n",
    "White Blood Cell Count\n",
    "\n",
    "There is a label of 0 or 1 indicating Not-Septic or Septic\n",
    "\n",
    "We will build a model which predicts sepsis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6d52d-f714-4f48-aa4f-3ee90fc0f4b7",
   "metadata": {},
   "source": [
    "## View the data\n",
    "\n",
    "You should be able to see five values. For example, the first example means patient is not septic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72d49fd-85ab-4df6-8782-60d9a737ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR,Temp,Resp,WBC,isSeptic\n",
      "40.0,110.0,12.0,4.54,0\n",
      "41.5,79.0,26.0,4.23,0\n",
      "41.9,61.0,14.0,18.13,0\n",
      "40.1,89.0,20.0,3.40,0\n",
      "35.6,136.0,13.0,9.21,0\n",
      "35.8,83.0,26.0,8.95,0\n",
      "35.6,83.0,18.0,21.70,0\n",
      "35.4,85.0,14.0,3.92,0\n",
      "38.3,238.0,25.0,6.96,0\n"
     ]
    }
   ],
   "source": [
    "!head {RAW_DATA}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e9dc8-d78b-49d4-8fdf-cd8a53846f47",
   "metadata": {},
   "source": [
    "## Write model training code\n",
    "\n",
    "This model training code will be saved to a separate file.\n",
    "\n",
    "In this tutorial we will use Generic Trainer of TFX which support Keras-based models. You need to write a Python file containing run_fn function, which is the entrypoint for the Trainer component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fc03c1-adcb-4263-9258-988b46f2f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_vitals_trainer_module_file = '../src/vitals_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58645cd-e656-441e-bb90-1e4e31e67505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/vitals_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_vitals_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'HR', 'Temp', 'Resp', 'WBC'\n",
    "]\n",
    "\n",
    "_LABEL_KEY = 'isSeptic'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying patient data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # save a model's architecture, weights, and training configuration in a single file/folder\n",
    "  # This allows you to export a model so it can be used without access to the original Python code*. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(\n",
    "      # output the trained model to a the desired location given by FnArgs\n",
    "      fn_args.serving_model_dir, \n",
    "      save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa42ad-d6e7-497b-8f19-546db8d0d7e1",
   "metadata": {},
   "source": [
    "## Write a pipeline definition\n",
    "\n",
    "We define a function to create a TFX pipeline. A Pipeline object represents a TFX pipeline which can be run using one of pipeline orchestration systems that TFX supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608dfcfa-dbcb-4635-bb60-a07c3793a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pipeline_file = '../src/pipeline.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6cc945-5c97-4de8-a135-a8f53fc38568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commented out writefile due to function not loading inside notebook\n",
    "#%%writefile {_pipeline_file}\n",
    "\n",
    "def _create_pipeline(pipeline_name: str,\n",
    "                     pipeline_root: str,\n",
    "                     data_root: str,\n",
    "                     module_file: str,\n",
    "                     serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component patient pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=DATA_TRAIN)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=os.path.abspath(_vitals_trainer_module_file),\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=SERVING_MODEL_DIR)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d585b-08a1-48e3-bf82-1e5857c3b491",
   "metadata": {},
   "source": [
    "# Run the pipeline\n",
    "\n",
    "You should see \"INFO:absl:Component Pusher is finished.\" at the end of the logs if the pipeline finished successfully. Because Pusher component is the last component of the pipeline.\n",
    "\n",
    "The pusher component pushes the trained model to the SERVING_MODEL_DIR which is the models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbff2df-f5e2-40d7-9485-0a8e50423679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if .ipynb_checkpoints exists, then pipeline will error due to split header mismatch\n",
    "!rm -rf ../data/{training_data,serving_data}/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e034831-256f-46e6-9c71-e811ab244e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for '/opt/app-root/src/mlops-basic/src/vitals_trainer.py' (including modules: ['vitals_trainer', 'vitals_constants', 'vitals_transform']).\n",
      "INFO:absl:User module package has hash fingerprint version 7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629.\n",
      "INFO:absl:Executing: ['/opt/app-root/bin/python3.8', '/tmp/tmphj94c967/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmponoto1xg', '--dist-dir', '/tmp/tmp1f9v_fgl']\n",
      "INFO:absl:Successfully built user code wheel distribution at '../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl'; target user module is 'vitals_trainer'.\n",
      "INFO:absl:Full user module path is 'vitals_trainer@../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"../pipeline/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"../pipeline/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"../data/training_data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 4\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/CsvExampleGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2389061,xor_checksum:1658278245,sum_checksum:1658278245\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_base': '../data/training_data', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:2389061,xor_checksum:1658278245,sum_checksum:1658278245'}, execution_output_uri='../pipeline/basic_pipeline/CsvExampleGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='../pipeline/basic_pipeline/CsvExampleGen/.system/stateful_working_dir/2022-07-21T00:24:47.584952', tmp_dir='../pipeline/basic_pipeline/CsvExampleGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"../data/training_data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"basic_pipeline\"\n",
      ", pipeline_run_id='2022-07-21T00:24:47.584952')\n",
      "INFO:absl:Generating examples.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processing input csv data ../data/training_data/* to TFExample.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 4 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/CsvExampleGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2389061,xor_checksum:1658278245,sum_checksum:1658278245\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 4\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-21T00:24:47.584952\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"vitals_trainer@../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 5\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'examples': [Artifact(artifact: id: 5\n",
      "type_id: 15\n",
      "uri: \"../pipeline/basic_pipeline/CsvExampleGen/examples/4\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2389061,xor_checksum:1658278245,sum_checksum:1658278245\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:CsvExampleGen:examples:0\"\n",
      "create_time_since_epoch: 1658363149151\n",
      "last_update_time_since_epoch: 1658363149151\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Trainer/model/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Trainer/model_run/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model_run:0\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'vitals_trainer@../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}'}, execution_output_uri='../pipeline/basic_pipeline/Trainer/.system/executor_execution/5/executor_output.pb', stateful_working_dir='../pipeline/basic_pipeline/Trainer/.system/stateful_working_dir/2022-07-21T00:24:47.584952', tmp_dir='../pipeline/basic_pipeline/Trainer/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-21T00:24:47.584952\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"vitals_trainer@../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"basic_pipeline\"\n",
      ", pipeline_run_id='2022-07-21T00:24:47.584952')\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'vitals_trainer@../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}'} 'run_fn'\n",
      "INFO:absl:Installing '../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['/opt/app-root/bin/python3.8', '-m', 'pip', 'install', '--target', '/tmp/tmp6vh5tr_e', '../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed '../pipeline/basic_pipeline/_wheels/tfx_user_code_Trainer-0.0+7a4f88ddd355534f2c448d0aaf5e38d35e174b568cbfa061cdee8ec483f6e629-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature HR has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Resp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Temp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature WBC has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature isSeptic has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature HR has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Resp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Temp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature WBC has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature isSeptic has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature HR has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Resp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Temp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature WBC has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature isSeptic has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature HR has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Resp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature Temp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature WBC has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature isSeptic has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Model: \"model\"\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl: Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl: HR (InputLayer)                [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: Temp (InputLayer)              [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: Resp (InputLayer)              [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: WBC (InputLayer)               [(None, 1)]          0           []                               \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: concatenate (Concatenate)      (None, 4)            0           ['HR[0][0]',                     \n",
      "INFO:absl:                                                                  'Temp[0][0]',                   \n",
      "INFO:absl:                                                                  'Resp[0][0]',                   \n",
      "INFO:absl:                                                                  'WBC[0][0]']                    \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense (Dense)                  (None, 8)            40          ['concatenate[0][0]']            \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_1 (Dense)                (None, 8)            72          ['dense[0][0]']                  \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl: dense_2 (Dense)                (None, 3)            27          ['dense_1[0][0]']                \n",
      "INFO:absl:                                                                                                  \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Total params: 139\n",
      "INFO:absl:Trainable params: 139\n",
      "INFO:absl:Non-trainable params: 0\n",
      "INFO:absl:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 4s 20ms/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) HR, Temp, Resp, WBC with unsupported characters which will be renamed to hr, temp, resp, wbc in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pipeline/basic_pipeline/Trainer/model/5/Format-Serving/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pipeline/basic_pipeline/Trainer/model/5/Format-Serving/assets\n",
      "INFO:absl:Training complete. Model written to ../pipeline/basic_pipeline/Trainer/model/5/Format-Serving. ModelRun written to ../pipeline/basic_pipeline/Trainer/model_run/5\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 5 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Trainer/model/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Trainer/model_run/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model_run:0\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}) for execution 5\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-21T00:24:47.584952\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"../models/sepsis\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ContextQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 6\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'model': [Artifact(artifact: id: 6\n",
      "type_id: 17\n",
      "uri: \"../pipeline/basic_pipeline/Trainer/model/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Trainer:model:0\"\n",
      "create_time_since_epoch: 1658363161868\n",
      "last_update_time_since_epoch: 1658363161868\n",
      ", artifact_type: id: 17\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Pusher/pushed_model/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Pusher:pushed_model:0\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"../models/sepsis\"\\n  }\\n}'}, execution_output_uri='../pipeline/basic_pipeline/Pusher/.system/executor_execution/6/executor_output.pb', stateful_working_dir='../pipeline/basic_pipeline/Pusher/.system/stateful_working_dir/2022-07-21T00:24:47.584952', tmp_dir='../pipeline/basic_pipeline/Pusher/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-21T00:24:47.584952\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"basic_pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-21T00:24:47.584952\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"basic_pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"../models/sepsis\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"basic_pipeline\"\n",
      ", pipeline_run_id='2022-07-21T00:24:47.584952')\n",
      "WARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\n",
      "INFO:absl:Model version: 1658363161\n",
      "INFO:absl:Model written to serving path ../models/sepsis/1658363161.\n",
      "INFO:absl:Model pushed to ../pipeline/basic_pipeline/Pusher/pushed_model/6.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 6 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"../pipeline/basic_pipeline/Pusher/pushed_model/6\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"basic_pipeline:2022-07-21T00:24:47.584952:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "name: \"basic_pipeline:2022-07-21T00:24:47.584952:Pusher:pushed_model:0\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 6\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_TRAIN,\n",
    "      module_file=_vitals_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71197d96-1688-4dfd-b3d4-18852021b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/sepsis:\n",
      "1658362528  1658362900\t1658363161\n",
      "\n",
      "../models/sepsis/1658362528:\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\n",
      "\n",
      "../models/sepsis/1658362528/assets:\n",
      "\n",
      "../models/sepsis/1658362528/variables:\n",
      "variables.data-00000-of-00001  variables.index\n",
      "\n",
      "../models/sepsis/1658362900:\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\n",
      "\n",
      "../models/sepsis/1658362900/assets:\n",
      "\n",
      "../models/sepsis/1658362900/variables:\n",
      "variables.data-00000-of-00001  variables.index\n",
      "\n",
      "../models/sepsis/1658363161:\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\n",
      "\n",
      "../models/sepsis/1658363161/assets:\n",
      "\n",
      "../models/sepsis/1658363161/variables:\n",
      "variables.data-00000-of-00001  variables.index\n"
     ]
    }
   ],
   "source": [
    "# List files in created model directory.\n",
    "!ls -R {SERVING_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b4478-6e00-428d-aa72-4f16023d1985",
   "metadata": {},
   "source": [
    "# BulkInferrer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d228717-7b20-43ec-bfec-549a0f2e697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at ../pipeline/basic_pipeline/metadata.sqlite.\n"
     ]
    }
   ],
   "source": [
    "# Here, we create an InteractiveContext using default parameters. This will\n",
    "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
    "# To use your own pipeline root or database, the optional properties\n",
    "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
    "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
    "# notebook.\n",
    "context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd7da09-5c62-48ff-bc94-a565658b8cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp,HR,RR,WBC\n",
      "39.37,82.65,15.97,6.23\n",
      "39.88,91.09,19.25,14.43"
     ]
    }
   ],
   "source": [
    "INFERENCE_ROOT='../data/serving_data'\n",
    "INFERENCE_EXAMPLES = os.path.join(INFERENCE_ROOT, 'septic_data_unlabelled.csv')\n",
    "!head {INFERENCE_EXAMPLES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e0131d-6861-47d1-9cd2-9fc13feab18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:Running executor for CsvExampleGen\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data ../data/serving_data/* to TFExample.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object expanded\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">ExecutionResult</span><span class=\"deemphasize\"> at 0x7fde5439baf0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.execution_id</td><td class = \"attrvalue\">2</td></tr><tr><td class=\"attr-name\">.component</td><td class = \"attrvalue\"><style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">CsvExampleGen</span><span class=\"deemphasize\"> at 0x7fddf562dbe0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.inputs</td><td class = \"attrvalue\">{}</td></tr><tr><td class=\"attr-name\">.outputs</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['examples']</td><td class = \"attrvalue\"><style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Channel</span> of type <span class=\"class-name\">'Examples'</span> (1 artifact)<span class=\"deemphasize\"> at 0x7fddf560d7f0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type_name</td><td class = \"attrvalue\">Examples</td></tr><tr><td class=\"attr-name\">._artifacts</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">[0]</td><td class = \"attrvalue\"><style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Artifact</span> of type <span class=\"class-name\">'Examples'</span> (uri: ../pipeline/basic_pipeline/CsvExampleGen/examples/2)<span class=\"deemphasize\"> at 0x7fde54272bb0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type</td><td class = \"attrvalue\">&lt;class &#x27;tfx.types.standard_artifacts.Examples&#x27;&gt;</td></tr><tr><td class=\"attr-name\">.uri</td><td class = \"attrvalue\">../pipeline/basic_pipeline/CsvExampleGen/examples/2</td></tr><tr><td class=\"attr-name\">.span</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">.split_names</td><td class = \"attrvalue\">[&quot;unlabelled&quot;]</td></tr><tr><td class=\"attr-name\">.version</td><td class = \"attrvalue\">0</td></tr></table></div></td></tr></table></td></tr></table></div></td></tr></table></td></tr><tr><td class=\"attr-name\">.exec_properties</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['input_base']</td><td class = \"attrvalue\">../data/serving_data</td></tr><tr><td class=\"attr-name\">['input_config']</td><td class = \"attrvalue\">{\n",
       "  &quot;splits&quot;: [\n",
       "    {\n",
       "      &quot;name&quot;: &quot;single_split&quot;,\n",
       "      &quot;pattern&quot;: &quot;*&quot;\n",
       "    }\n",
       "  ]\n",
       "}</td></tr><tr><td class=\"attr-name\">['output_config']</td><td class = \"attrvalue\">{\n",
       "  &quot;split_config&quot;: {\n",
       "    &quot;splits&quot;: [\n",
       "      {\n",
       "        &quot;hash_buckets&quot;: 1,\n",
       "        &quot;name&quot;: &quot;unlabelled&quot;\n",
       "      }\n",
       "    ]\n",
       "  }\n",
       "}</td></tr><tr><td class=\"attr-name\">['output_data_format']</td><td class = \"attrvalue\">6</td></tr><tr><td class=\"attr-name\">['output_file_format']</td><td class = \"attrvalue\">5</td></tr><tr><td class=\"attr-name\">['custom_config']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['range_config']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['span']</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">['version']</td><td class = \"attrvalue\">None</td></tr><tr><td class=\"attr-name\">['input_fingerprint']</td><td class = \"attrvalue\">split:single_split,num_files:1,total_bytes:63,xor_checksum:1658357963,sum_checksum:1658357963</td></tr></table></td></tr></table></div></td></tr><tr><td class=\"attr-name\">.component.inputs</td><td class = \"attrvalue\">{}</td></tr><tr><td class=\"attr-name\">.component.outputs</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">['examples']</td><td class = \"attrvalue\"><style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Channel</span> of type <span class=\"class-name\">'Examples'</span> (1 artifact)<span class=\"deemphasize\"> at 0x7fddf560d7f0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type_name</td><td class = \"attrvalue\">Examples</td></tr><tr><td class=\"attr-name\">._artifacts</td><td class = \"attrvalue\"><table class=\"attr-table\"><tr><td class=\"attr-name\">[0]</td><td class = \"attrvalue\"><style>\n",
       ".tfx-object.expanded {\n",
       "  padding: 4px 8px 4px 8px;\n",
       "  background: white;\n",
       "  border: 1px solid #bbbbbb;\n",
       "  box-shadow: 4px 4px 2px rgba(0,0,0,0.05);\n",
       "}\n",
       "html[theme=dark] .tfx-object.expanded {\n",
       "  background: black;\n",
       "}\n",
       ".tfx-object, .tfx-object * {\n",
       "  font-size: 11pt;\n",
       "}\n",
       ".tfx-object > .title {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".tfx-object .expansion-marker {\n",
       "  color: #999999;\n",
       "}\n",
       ".tfx-object.expanded > .title > .expansion-marker:before {\n",
       "  content: '▼';\n",
       "}\n",
       ".tfx-object.collapsed > .title > .expansion-marker:before {\n",
       "  content: '▶';\n",
       "}\n",
       ".tfx-object .class-name {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object .deemphasize {\n",
       "  opacity: 0.5;\n",
       "}\n",
       ".tfx-object.collapsed > table.attr-table {\n",
       "  display: none;\n",
       "}\n",
       ".tfx-object.expanded > table.attr-table {\n",
       "  display: block;\n",
       "}\n",
       ".tfx-object table.attr-table {\n",
       "  border: 2px solid white;\n",
       "  margin-top: 5px;\n",
       "}\n",
       "html[theme=dark] .tfx-object table.attr-table {\n",
       "  border: 2px solid black;\n",
       "}\n",
       ".tfx-object table.attr-table td.attr-name {\n",
       "  vertical-align: top;\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tfx-object table.attr-table td.attrvalue {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "function toggleTfxObject(element) {\n",
       "  var objElement = element.parentElement;\n",
       "  if (objElement.classList.contains('collapsed')) {\n",
       "    objElement.classList.remove('collapsed');\n",
       "    objElement.classList.add('expanded');\n",
       "  } else {\n",
       "    objElement.classList.add('collapsed');\n",
       "    objElement.classList.remove('expanded');\n",
       "  }\n",
       "}\n",
       "</script>\n",
       "<div class=\"tfx-object collapsed\"><div class = \"title\" onclick=\"toggleTfxObject(this)\"><span class=\"expansion-marker\"></span><span class=\"class-name\">Artifact</span> of type <span class=\"class-name\">'Examples'</span> (uri: ../pipeline/basic_pipeline/CsvExampleGen/examples/2)<span class=\"deemphasize\"> at 0x7fde54272bb0</span></div><table class=\"attr-table\"><tr><td class=\"attr-name\">.type</td><td class = \"attrvalue\">&lt;class &#x27;tfx.types.standard_artifacts.Examples&#x27;&gt;</td></tr><tr><td class=\"attr-name\">.uri</td><td class = \"attrvalue\">../pipeline/basic_pipeline/CsvExampleGen/examples/2</td></tr><tr><td class=\"attr-name\">.span</td><td class = \"attrvalue\">0</td></tr><tr><td class=\"attr-name\">.split_names</td><td class = \"attrvalue\">[&quot;unlabelled&quot;]</td></tr><tr><td class=\"attr-name\">.version</td><td class = \"attrvalue\">0</td></tr></table></div></td></tr></table></td></tr></table></div></td></tr></table></td></tr></table></div>"
      ],
      "text/plain": [
       "ExecutionResult(\n",
       "    component_id: CsvExampleGen\n",
       "    execution_id: 2\n",
       "    outputs:\n",
       "        examples: OutputChannel(artifact_type=Examples, producer_component_id=CsvExampleGen, output_key=examples, additional_properties={}, additional_custom_properties={}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tfx.proto import example_gen_pb2\n",
    "\n",
    "output = tfx.proto.Output(\n",
    "             split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "                 tfx.proto.SplitConfig.Split(name='unlabelled', hash_buckets=1)             ]))\n",
    "\n",
    "inference_example_gen = tfx.components.CsvExampleGen(\n",
    "    # input_base an external directory containing the CSV files.\n",
    "    input_base=INFERENCE_ROOT, \n",
    "    # To customize the train/eval split ratio which ExampleGen will output, set the output_config for ExampleGen component. \n",
    "    output_config=output)\n",
    "context.run(inference_example_gen, enable_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c7e62c3-3c4c-4068-a261-7eb8223af465",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ba363daeb5c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m bulk_inferrer = tfx.components.BulkInferrer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_example_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'examples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_blessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blessing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unlabelled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/tensorflow/tfx/issues/2478#issuecomment-770373362\n",
    "\n",
    "bulk_inferrer = tfx.components.BulkInferrer(\n",
    "    examples=inference_example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=evaluator.outputs['blessing'],\n",
    "    data_spec=tfx.proto.DataSpec(example_splits=['unlabelled']),\n",
    "    model_spec=tfx.proto.ModelSpec())\n",
    "\n",
    "context.run(bulk_inferrer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86150b-ab8b-4c42-9656-277c8a050926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "# train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "inference_uri = bulk_inferrer.outputs['inference_result'].get()[0].uri\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(inference_uri, name)\n",
    "                      for name in os.listdir(inference_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 1 records and decode them.\n",
    "for tfrecord in dataset.take(2):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  inference_example = tf.train.Example()\n",
    "  inference_example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3c63c-3183-4991-9356-0ad62d246898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "# train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "inference_uri = bulk_inferrer.outputs['inference_result'].get()[0].uri\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(inference_uri, name)\n",
    "                      for name in os.listdir(inference_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 1 records and decode them.\n",
    "for tfrecord in dataset.take(2):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  inference_example = tf.train.Example()\n",
    "  inference_example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c6761-580a-409f-89e9-dd9626fd5c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
